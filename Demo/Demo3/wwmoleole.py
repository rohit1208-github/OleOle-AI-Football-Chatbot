# -*- coding: utf-8 -*-
"""WWMoleole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JOYPq-NyhZOxxmKCgjbcaLndgfLLlTF7
"""

import os
if "COLAB_GPU" in os.environ:
    print("[INFO] Running in Google Colab, installing requirements.")
    !pip install -U torch
    !pip install PyMuPDF
    !pip install tqdm
    !pip install sentence-transformers
    !pip install accelerate
    !pip install bitsandbytes
    !pip install flash-attn --no-build-isolation

import os
import requests
pdf_path = "/content/sodapdf-converted.pdf"

import fitz
from tqdm.auto import tqdm

def text_formatter(text: str) -> str:
    cleaned_text = text.replace("\n", " ").strip()
    return cleaned_text

def open_and_read_pdf(pdf_path: str) -> list[dict]:
    doc = fitz.open(pdf_path)
    pages_and_texts = []
    for page_number, page in tqdm(enumerate(doc)):
        text = page.get_text()
        text = text_formatter(text)
        pages_and_texts.append({"page_number": page_number - 0,
                                "page_char_count": len(text),
                                "page_word_count": len(text.split(" ")),
                                "page_sentence_count_raw": len(text.split(". ")),
                                "page_token_count": len(text) / 4,
                                "text": text})
    return pages_and_texts
pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)
pages_and_texts[:2]

import random
random.sample(pages_and_texts, k=3)

from spacy.lang.en import English
nlp = English()
nlp.add_pipe("sentencizer")

for item in tqdm(pages_and_texts):
    item["sentences"] = list(nlp(item["text"]).sents)
    item["sentences"] = [str(sentence) for sentence in item["sentences"]]
    item["page_sentence_count_spacy"] = len(item["sentences"])

num_sentence_chunk_size = 23
def split_list(input_list: list,
               slice_size: int) -> list[list[str]]:
    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]
for item in tqdm(pages_and_texts):
    item["sentence_chunks"] = split_list(input_list=item["sentences"],
                                         slice_size=num_sentence_chunk_size)
    item["num_chunks"] = len(item["sentence_chunks"])

random.sample(pages_and_texts, k=1)

import re
pages_and_chunks = []
for item in tqdm(pages_and_texts):
    for sentence_chunk in item["sentence_chunks"]:
        chunk_dict = {}
        chunk_dict["page_number"] = item["page_number"]
        joined_sentence_chunk = "".join(sentence_chunk).replace("  ", " ").strip()
        joined_sentence_chunk = re.sub(r'\.([A-Z])', r'. \1', joined_sentence_chunk)
        chunk_dict["sentence_chunk"] = joined_sentence_chunk
        chunk_dict["chunk_char_count"] = len(joined_sentence_chunk)
        chunk_dict["chunk_word_count"] = len([word for word in joined_sentence_chunk.split(" ")])
        chunk_dict["chunk_token_count"] = len(joined_sentence_chunk) / 4
        pages_and_chunks.append(chunk_dict)
len(pages_and_chunks)

random.sample(pages_and_chunks, k=1)

import pandas as pd

df = pd.DataFrame(pages_and_chunks)
df.describe().round(2)

min_token_length = 90
for row in df[df["chunk_token_count"] <= min_token_length].sample(5).iterrows():
    print(f'Chunk token count: {row[1]["chunk_token_count"]} | Text: {row[1]["sentence_chunk"]}')

pages_and_chunks_over_min_token_len = df[df["chunk_token_count"] > min_token_length].to_dict(orient="records")
pages_and_chunks_over_min_token_len[:2]

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2",
                                      device="cuda")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# embedding_model.to("cuda")
# for item in tqdm(pages_and_chunks_over_min_token_len):
#     item["embedding"] = embedding_model.encode(item["sentence_chunk"])

text_chunks = [item["sentence_chunk"] for item in pages_and_chunks_over_min_token_len]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# text_chunk_embeddings = embedding_model.encode(text_chunks,
#                                                batch_size=512,
#                                                convert_to_tensor=True)
# text_chunk_embeddings

text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)
embeddings_df_save_path = "text_chunks_and_embeddings_df.csv"
text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False, escapechar='\\')

text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)
text_chunks_and_embedding_df_load.tail()

import random
import torch
import numpy as np
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"
text_chunks_and_embedding_df = pd.read_csv("/content/text_chunks_and_embeddings_df.csv")
text_chunks_and_embedding_df["embedding"] = text_chunks_and_embedding_df["embedding"].apply(lambda x: np.fromstring(x.strip("[]"), sep=" "))
pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient="records")
embeddings = torch.tensor(np.array(text_chunks_and_embedding_df["embedding"].tolist()), dtype=torch.float32).to(device)
embeddings.shape

from sentence_transformers import util

query = "Who is Thierry Henry"
print(f"Query: {query}")
query_embedding = embedding_model.encode(query, convert_to_tensor=True)
from time import perf_counter as timer

start_time = timer()
dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]
end_time = timer()

print(f"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

top_results_dot_product = torch.topk(dot_scores, k=5)
top_results_dot_product

larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)
print(f"Embeddings shape: {larger_embeddings.shape}")

start_time = timer()
dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]
end_time = timer()

print(f"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

import textwrap

def print_wrapped(text, wrap_length=80):
    wrapped_text = textwrap.fill(text, wrap_length)
    print(wrapped_text)

print(f"Query: '{query}'\n")
print("Results:")
for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):
    print(f"Score: {score:.4f}")
    print("Text:")
    print_wrapped(pages_and_chunks[idx]["sentence_chunk"])
    print(f"Page number: {pages_and_chunks[idx]['page_number']}")
    print("\n")

def retrieve_relevant_resources(query: str,
                                embeddings: torch.tensor,
                                model: SentenceTransformer=embedding_model,
                                n_resources_to_return: int=13,  # Update the default value to 15
                                print_time: bool=True):
    query_embedding = model.encode(query,
                                   convert_to_tensor=True)
    start_time = timer()
    dot_scores = util.dot_score(query_embedding, embeddings)[0]
    end_time = timer()

    if print_time:
        print(f"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

    scores, indices = torch.topk(input=dot_scores,
                                 k=n_resources_to_return)

    return scores, indices

def print_top_results_and_scores(query: str,
                                 embeddings: torch.tensor,
                                 pages_and_chunks: list[dict]=pages_and_chunks,
                                 n_resources_to_return: int=5):
    scores, indices = retrieve_relevant_resources(query=query,
                                                  embeddings=embeddings,
                                                  n_resources_to_return=n_resources_to_return)
    print(f"Query: {query}\n")
    print("Results:")
    for score, index in zip(scores, indices):
        print(f"Score: {score:.4f}")
        print_wrapped(pages_and_chunks[index]["sentence_chunk"])
        print(f"Page number: {pages_and_chunks[index]['page_number']}")
        print("\n")

query = "Cups of Arsenal"
scores, indices = retrieve_relevant_resources(query=query,
                                              embeddings=embeddings)
scores, indices

print_top_results_and_scores(query=query,
                             embeddings=embeddings)

use_quantization_config = False
model_id = "google/gemma-2b-it"

!huggingface-cli login

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import is_flash_attn_2_available
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(load_in_4bit=True,
                                         bnb_4bit_compute_dtype=torch.float16)
if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):
  attn_implementation = "flash_attention_2"
else:
  attn_implementation = "sdpa"
print(f"[INFO] Using attention implementation: {attn_implementation}")
model_id = model_id
print(f"[INFO] Using model_id: {model_id}")
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)
llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,
                                                 torch_dtype=torch.float16,
                                                 quantization_config=quantization_config if use_quantization_config else None,
                                                 low_cpu_mem_usage=False,
                                                 attn_implementation=attn_implementation)

if not use_quantization_config:
    llm_model.to("cuda")

llm_model

def get_model_num_params(model: torch.nn.Module):
    return sum([param.numel() for param in model.parameters()])

get_model_num_params(llm_model)

def get_model_mem_size(model: torch.nn.Module):
    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])
    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])
    model_mem_bytes = mem_params + mem_buffers
    model_mem_mb = model_mem_bytes / (1024**2)
    model_mem_gb = model_mem_bytes / (1024**3)

    return {"model_mem_bytes": model_mem_bytes,
            "model_mem_mb": round(model_mem_mb, 2),
            "model_mem_gb": round(model_mem_gb, 2)}
get_model_mem_size(llm_model)

input_text = "Tell me the history of the Arsenal football club by the decade from their inception in 1886. Include the trophies they won each season who the managers were and who are the legends that have played for Arsenal football club. Also include the history at Highbury and at the Emirates stadium."
print(f"Input text:\n{input_text}")
dialogue_template = [
    {"role": "user",
     "content": input_text}
]
prompt = tokenizer.apply_chat_template(conversation=dialogue_template,
                                       tokenize=False,
                                       add_generation_prompt=True)
print(f"\nPrompt (formatted):\n{prompt}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
# print(f"Model input (tokenized):\n{input_ids}\n")
# outputs = llm_model.generate(**input_ids,
#                              max_new_tokens=4096)
# print(f"Model output (tokens):\n{outputs[0]}\n")

outputs_decoded = tokenizer.decode(outputs[0])
print(f"Model output (decoded):\n{outputs_decoded}\n")

print(f"Input text: {input_text}\n")
print(f"Output text:\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}")

query_list = ["Tell me all about Arsenal history"]

query = random.choice(query_list)
print(f"Query: {query}")

# Get relevant resources
scores, indices = retrieve_relevant_resources(query=query,
                                              embeddings=embeddings)

# Create a list of context items with scores
context_items = [pages_and_chunks[i] for i in indices]
for i, item in enumerate(context_items):
    item["score"] = scores[i].cpu().item()

def prompt_formatter(query: str,
                     context_items: list[dict]) -> str:
    context_with_scores = []
    for item in context_items:
        context_with_scores.append(f"[Score: {item['score']:.4f}, Page: {item['page_number']}] {item['sentence_chunk']}")
    context = " \n ".join(context_with_scores)

    base_prompt = """Generate an extremely comprehensive and exhaustive answer on the history of the Arsenal football club.

    Your response should be at least 5000 words long, divided into multiple detailed paragraphs, each containing 10-15 sentences. Provide in-depth and extensive details on every aspect of Arsenal's history, including but not limited to:

    - Founding and early years, including the club's origins and initial struggles
    - Comprehensive list of major achievements and trophies won throughout the club's history, with details on significant matches and seasons
    - Biographical information and achievements of legendary players and managers, their impact on the club's success, and their legacy
    - Detailed accounts of significant events, matches, and rivalries that shaped the club's history, including memorable goals, comebacks, and controversies
    - Comprehensive history of the club's stadiums, including Highbury and Emirates, with details on their construction, renovations, and cultural significance
    - In-depth overview of the club's youth academy and player development system, highlighting notable players who came through the ranks
    - Analysis of the club's global fanbase, cultural impact, and influence on the sport beyond just their on-field achievements

    Make sure to thoroughly cover all the information available in the context passages below, and supplement it with your own extensive knowledge and insights. Use a formal, authoritative, and informative writing style, with well-researched facts and statistics. Ensure that your sentences are well-structured, varied in length, and flow coherently.

    \nNow take everything as context from the answer and use the following context items to answer the user query:
    {context}
    \nRelevant passages: <extract relevant passages from the context here, including the page numbers>
    User query: {query}
    Answer:"""
    base_prompt = base_prompt.format(context=context, query=query)
    dialogue_template = [
        {"role": "user",
         "content": base_prompt}
    ]
    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,
                                           tokenize=False,
                                           add_generation_prompt=True)
    return prompt

query = random.choice(query_list)
print(f"Query: {query}")

# Get relevant resources
scores, indices = retrieve_relevant_resources(query=query,
                                              embeddings=embeddings,
                                              n_resources_to_return=13)  # Specify the desired number of sources

# Create a list of context items with scores
context_items = [pages_and_chunks[i] for i in indices]
for i, item in enumerate(context_items):
    item["score"] = scores[i].cpu().item()

# Print the number of sources being used
print(f"Number of sources used: {len(context_items)}")

# Format prompt with context items
prompt = prompt_formatter(query=query,
                          context_items=context_items)
print(prompt)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
# outputs = llm_model.generate(**input_ids,
#                              temperature=0.7,
#                              do_sample=True,
#                              max_new_tokens=9000)
# output_text = tokenizer.decode(outputs[0])
# print(f"Query: {query}")
# print(f"RAG answer:\n{output_text.replace(prompt, '')}")

def ask(query,
        temperature=0.7,
        max_new_tokens=9000,
        format_answer_text=True,
        return_answer_only=True):
    scores, indices = retrieve_relevant_resources(query=query,
                                                  embeddings=embeddings)
    context_items = [pages_and_chunks[i] for i in indices]
    for i, item in enumerate(context_items):
        item["score"] = scores[i].cpu()
    prompt = prompt_formatter(query=query,
                              context_items=context_items)
    input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = llm_model.generate(**input_ids,
                                 temperature=temperature,
                                 do_sample=True,
                                 max_new_tokens=max_new_tokens)
    output_text = tokenizer.decode(outputs[0])

    if format_answer_text:
        answer_sentences = output_text.replace(prompt, "").replace("<bos>", "").replace("<eos>", "").split(". ")
        formatted_answer = []
        for sentence in answer_sentences:
            sentence = sentence.strip() + ". "
            if sentence:
                for item in context_items:
                    if sentence in item["sentence_chunk"]:
                        formatted_answer.append(f"[Source: Page {item['page_number']}] {sentence}")
                        break
                else:
                    formatted_answer.append(f"[Source: Generated] {sentence}")
        output_text = " ".join(formatted_answer)

    if return_answer_only:
        return output_text

    return output_text, context_items

query = random.choice(query_list)
print(f"Query: {query}")
answer, context_items = ask(query=query,
                            temperature=0.7,
                            max_new_tokens=4096,
                            return_answer_only=False)
print(f"Answer:\n")
print_wrapped(answer)