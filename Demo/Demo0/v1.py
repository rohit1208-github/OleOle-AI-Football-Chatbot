# -*- coding: utf-8 -*-
"""v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MB70X06wM2zyOfvlqVS9_nzrnGwducxz
"""

!pip install accelerate==0.27.2

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
import requests
from bs4 import BeautifulSoup
import tempfile

def scrape_data(links):
    data = ""
    for link in links:
        response = requests.get(link)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        for paragraph in paragraphs:
            data += paragraph.text + "\n"
    return data

def prepare_dataset(data):
    tokenizer = AutoTokenizer.from_pretrained("gpt2")

    with tempfile.NamedTemporaryFile(mode="w", delete=False) as temp_file:
        temp_file.write(data)
        temp_file_path = temp_file.name

    dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=temp_file_path,
        block_size=128,
    )
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False
    )
    return dataset, data_collator

def train_model(dataset, data_collator):
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        save_steps=10_000,
        save_total_limit=2,
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
    )
    trainer.train()
    return model

def generate_summary(model, team_name):
    generator = pipeline("text-generation", model=model, tokenizer="gpt2")
    prompt = f"Provide a detailed summary of {team_name}:"
    summary = generator(prompt, max_length=1000, num_return_sequences=1)[0]["generated_text"]
    return summary

links = ["https://en.wikipedia.org/wiki/Arsenal_F.C.",
    "https://www.footballhistory.org/club/arsenal.html",
    "https://www.arsenalinsider.com/club/club-history/",
    "https://www.britannica.com/topic/Arsenal-English-football-club",
    "https://www.bbc.com/storyworks/top-teams-uncovered/arsenal",
    "https://www.arsenalfcyears.com/",
    "https://www.soccermaniak.com/arsenal-history.html"]

links = ["https://en.wikipedia.org/wiki/Manchester_United_F.C.",
    "https://www.footballhistory.org/club/manchester-united.html",
    "https://americanreddevils.com/the-history-of-manchester-united-football-club/",
    "https://theforkball.com/manchester-united-history-a-journey-through-time-and-trophies/",
    "https://spartacus-educational.com/FmanchesterU.htm",
    "https://www.reeditionmagazine.com/to-the-minute/the-story-of-the-man-utd-football-club",
    "https://www.zippia.com/manchester-united-careers-1573651/history/"]

scraped_data = scrape_data(links)
dataset, data_collator = prepare_dataset(scraped_data)
model = train_model(dataset, data_collator)

team_name = "Manchester United"
summary = generate_summary(model, team_name)

output_file = "summary.txt"
with open(output_file, "w") as file:
    file.write(summary)

print(f"Summary saved to {output_file}")

!cat summary.txt

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

messages = [
    {
        "role": "system",
        "content": "You are a chatbot answering football queries in detail",
    },
    {"role": "user", "content": "Tell me about Arsenal's achievements from 2000 to 2024"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])